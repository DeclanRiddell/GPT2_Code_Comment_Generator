{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASRCGPT2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTmrvmnvVxHR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79da82c-8439-4634-9630-e9bb090c5a17"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMtU-RHmsb5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b8b8188-9988-4b78-c3ee-11739f241102"
      },
      "source": [
        "import pandas as pd\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "from tqdm import tqdm, trange\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "\n",
        "comments = pd.read_csv('/content/drive/MyDrive/ASRCML/comments.csv')\n",
        "code = pd.read_csv('/content/drive/MyDrive/ASRCML/code.csv')\n",
        "\n",
        "#df = comments.merge(code, how=\"inner\")\n",
        "\n",
        "#Create a very small test set to compare generated text with the reality\n",
        "#test_set = df.sample(n=200)\n",
        "#df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "#Reset the indexes\n",
        "#test_set = test_set.reset_index()\n",
        "#df = df.reset_index()\n",
        "\n",
        "df = comments\n",
        "print(df)\n",
        "test_set = df.sample(n=200)\n",
        "df = df.loc[~df.index.isin(test_set.index)]\n",
        "\n",
        "#Reset the indexes\n",
        "test_set = test_set.reset_index()\n",
        "df = df.reset_index()\n",
        "\n",
        "print(test_set)\n",
        "#print(test_set['Comments'][0])\n",
        "\n",
        "class Comments(Dataset):  \n",
        "    def __init__(self, control_code, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
        "        self.comments = []\n",
        "\n",
        "        for row in df['Comments']:\n",
        "          self.comments.append(torch.tensor(\n",
        "                self.tokenizer.encode(f\"<|{control_code}|>{row[:max_length]}<|endoftext|>\")\n",
        "            ))               \n",
        "        if truncate:\n",
        "            self.comments = self.comments[:20000]\n",
        "        self.comments_count = len(self.comments)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.comments_count\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.comments[item]\n",
        "    \n",
        "\n",
        "\n",
        "#ready the dataset\n",
        "#dataset = SongLyrics(df['Lyric'], truncate=True, gpt2_type=\"gpt2\") \n",
        "\n",
        "#Get the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "#Accumulated batch size (since GPT2 is so big)\n",
        "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
        "    if packed_tensor is None:\n",
        "        return new_tensor, True, None\n",
        "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
        "        return packed_tensor, False, new_tensor\n",
        "    else:\n",
        "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
        "        return packed_tensor, True, None\n",
        "\n",
        "def train(\n",
        "    dataset, model, tokenizer,\n",
        "    batch_size=16, epochs=5, lr=2e-5,\n",
        "    max_seq_len=400, warmup_steps=200,\n",
        "    gpt2_type=\"gpt2\", output_dir=\".\", output_prefix=\"wreckgar\",\n",
        "    test_mode=False,save_model_on_epoch=False,\n",
        "):\n",
        "    acc_steps = 100\n",
        "    device=torch.device(\"cuda\")\n",
        "    model = model.cuda()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=lr)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
        "    )\n",
        "\n",
        "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "    loss=0\n",
        "    accumulating_batch_count = 0\n",
        "    input_tensor = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"Training epoch {epoch}\")\n",
        "        print(loss)\n",
        "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
        "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
        "\n",
        "            if carry_on and idx != len(train_dataloader) - 1:\n",
        "                continue\n",
        "\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            outputs = model(input_tensor, labels=input_tensor)\n",
        "            loss = outputs[0]\n",
        "            loss.backward()\n",
        "\n",
        "            if (accumulating_batch_count % batch_size) == 0:\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                optimizer.zero_grad()\n",
        "                model.zero_grad()\n",
        "\n",
        "            accumulating_batch_count += 1\n",
        "            input_tensor = None\n",
        "        if save_model_on_epoch:\n",
        "            torch.save(\n",
        "                model.state_dict(),\n",
        "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
        "            )\n",
        "    return model"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                 Comments Unnamed: 1\n",
            "0       /**     * Chu Li Mei Yi Tiao Ji Lu      * <b>Y...        NaN\n",
            "1       /**     * Fen Ye Chu Li      * @param batchPar...        NaN\n",
            "2       /* ---------------------- Ende der inneren Kla...        NaN\n",
            "3                    /**         * Konstruktor         */        NaN\n",
            "4       /**         * Diese Methode fugt ein neues Obj...        NaN\n",
            "...                                                   ...        ...\n",
            "547646                       /** * @return the regdate */        NaN\n",
            "547647         /** * @param regdate the regdate to set */        NaN\n",
            "547648                    /** * @return the grpRegDate */        NaN\n",
            "547649   /** * @param grpRegDate the grpRegDate to set */        NaN\n",
            "547650                        /** * @return the grpCnt */        NaN\n",
            "\n",
            "[547651 rows x 2 columns]\n",
            "      index  ...                                         Unnamed: 1\n",
            "0    155948  ...                                                NaN\n",
            "1     96982  ...                                                NaN\n",
            "2    130308  ...                                                NaN\n",
            "3    429966  ...                                                NaN\n",
            "4    196813  ...                                                NaN\n",
            "..      ...  ...                                                ...\n",
            "195  282485  ...                                                NaN\n",
            "196   80036  ...                                                NaN\n",
            "197   68370  ...                                                NaN\n",
            "198  289082  ...  all chat, whoever it's from.     *         cha...\n",
            "199  217635  ...                                                NaN\n",
            "\n",
            "[200 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyn44N3-vAUE",
        "outputId": "85a37f30-ff72-4002-cb58-257b9c87d516"
      },
      "source": [
        "print(test_set['Comments'][0])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/**   * Registers a {@link Closeable} resource that should be closed after the suite   * completes.   *    * @return <code>resource</code> (for call chaining).   */\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdmPoPsi9EpG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "561d8a12-e3df-42a2-f930-ff2151d56a8b"
      },
      "source": [
        "def generate(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    entry_count=10,\n",
        "    entry_length=30, #maximum number of words\n",
        "    top_p=0.8,\n",
        "    temperature=1.,\n",
        "):\n",
        "    model.eval()\n",
        "    generated_num = 0\n",
        "    generated_list = []\n",
        "\n",
        "    filter_value = -float(\"Inf\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for entry_idx in trange(entry_count):\n",
        "\n",
        "            entry_finished = False\n",
        "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
        "\n",
        "            for i in range(entry_length):\n",
        "                outputs = model(generated, labels=generated)\n",
        "                loss, logits = outputs[:2]\n",
        "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
        "                    ..., :-1\n",
        "                ].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[:, indices_to_remove] = filter_value\n",
        "\n",
        "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
        "                generated = torch.cat((generated, next_token), dim=1)\n",
        "\n",
        "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
        "                    entry_finished = True\n",
        "\n",
        "                if entry_finished:\n",
        "\n",
        "                    generated_num = generated_num + 1\n",
        "\n",
        "                    output_list = list(generated.squeeze().numpy())\n",
        "                    output_text = tokenizer.decode(output_list)\n",
        "                    generated_list.append(output_text)\n",
        "                    break\n",
        "            \n",
        "            if not entry_finished:\n",
        "              output_list = list(generated.squeeze().numpy())\n",
        "              output_text = f\"{tokenizer.decode(output_list)}<|endoftext|>\" \n",
        "              generated_list.append(output_text)\n",
        "                \n",
        "    return generated_list\n",
        "\n",
        "def text_generation(test_data):\n",
        "  generated_comments = []\n",
        "  for i in range(len(test_data)):\n",
        "    x = generate(model.to('cpu'), tokenizer, test_data['Comments'][i], entry_count=1)\n",
        "    generated_comments.append(x)\n",
        "  return generated_comments\n",
        "\n",
        "generated_comments = text_generation(test_set)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:15<00:00, 15.57s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.64s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.50s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.62s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.50s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.62s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.80s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.39s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.93s/it]\n",
            "100%|██████████| 1/1 [00:21<00:00, 21.08s/it]\n",
            "100%|██████████| 1/1 [00:22<00:00, 22.23s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.08s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.50s/it]\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.41s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.22s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.49s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.85s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.48s/it]\n",
            "100%|██████████| 1/1 [00:31<00:00, 31.76s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.52s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.68s/it]\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.58s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.26s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.02s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.79s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.30s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.71s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.64s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.14s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.11s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.99s/it]\n",
            "100%|██████████| 1/1 [00:15<00:00, 15.75s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.02s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.81s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.08s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.42s/it]\n",
            "100%|██████████| 1/1 [00:32<00:00, 32.05s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.58s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.95s/it]\n",
            "100%|██████████| 1/1 [00:25<00:00, 25.47s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.54s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.76s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.16s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.02s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.57s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.33s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.55s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.44s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.27s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.23s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.26s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.50s/it]\n",
            "100%|██████████| 1/1 [00:22<00:00, 22.93s/it]\n",
            "100%|██████████| 1/1 [00:15<00:00, 15.92s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.45s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.72s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.29s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.44s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.98s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.85s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.09s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.86s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.88s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.95s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.55s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.19s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.71s/it]\n",
            "100%|██████████| 1/1 [00:22<00:00, 22.99s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.53s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.75s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.74s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.84s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.29s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.57s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.66s/it]\n",
            "100%|██████████| 1/1 [00:15<00:00, 15.40s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.15s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.63s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.21s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.93s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.27s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.94s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.80s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.04s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.40s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.90s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.53s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.05s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.06s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.27s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.35s/it]\n",
            "100%|██████████| 1/1 [01:04<00:00, 64.43s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.52s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.99s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.32s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.32s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.63s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.38s/it]\n",
            "100%|██████████| 1/1 [00:16<00:00, 16.32s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.27s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.03s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.34s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.01s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.95s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.74s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.09s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.81s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.66s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.13s/it]\n",
            "100%|██████████| 1/1 [00:19<00:00, 19.33s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.15s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.31s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.74s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.01s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.83s/it]\n",
            "100%|██████████| 1/1 [00:44<00:00, 44.77s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.18s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "100%|██████████| 1/1 [00:20<00:00, 20.76s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.45s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.86s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.78s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.85s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.01s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.86s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.03s/it]\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.06s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.54s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.98s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.53s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.14s/it]\n",
            "100%|██████████| 1/1 [00:54<00:00, 54.80s/it]\n",
            "100%|██████████| 1/1 [00:32<00:00, 32.43s/it]\n",
            "100%|██████████| 1/1 [00:16<00:00, 16.95s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.69s/it]\n",
            "100%|██████████| 1/1 [01:06<00:00, 66.37s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.85s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.59s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.58s/it]\n",
            "100%|██████████| 1/1 [00:05<00:00,  6.00s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.63s/it]\n",
            "100%|██████████| 1/1 [00:32<00:00, 32.58s/it]\n",
            "100%|██████████| 1/1 [00:18<00:00, 18.25s/it]\n",
            "100%|██████████| 1/1 [00:47<00:00, 47.91s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.14s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.41s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.73s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.79s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.90s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.05s/it]\n",
            "100%|██████████| 1/1 [00:27<00:00, 27.10s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.26s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.40s/it]\n",
            "100%|██████████| 1/1 [00:17<00:00, 17.62s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.94s/it]\n",
            "100%|██████████| 1/1 [00:14<00:00, 14.40s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.76s/it]\n",
            "100%|██████████| 1/1 [00:26<00:00, 26.39s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.12s/it]\n",
            "100%|██████████| 1/1 [00:20<00:00, 20.69s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.31s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.58s/it]\n",
            "100%|██████████| 1/1 [00:19<00:00, 19.87s/it]\n",
            "100%|██████████| 1/1 [00:10<00:00, 10.40s/it]\n",
            "100%|██████████| 1/1 [00:35<00:00, 35.40s/it]\n",
            "100%|██████████| 1/1 [00:16<00:00, 16.58s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.50s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.44s/it]\n",
            "100%|██████████| 1/1 [00:19<00:00, 19.08s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.82s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.71s/it]\n",
            "100%|██████████| 1/1 [00:58<00:00, 58.53s/it]\n",
            "100%|██████████| 1/1 [00:16<00:00, 16.25s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.00s/it]\n",
            "100%|██████████| 1/1 [00:12<00:00, 12.98s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.13s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.90s/it]\n",
            "100%|██████████| 1/1 [00:28<00:00, 28.18s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.95s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.97s/it]\n",
            "100%|██████████| 1/1 [00:29<00:00, 29.56s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.99s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.73s/it]\n",
            "100%|██████████| 1/1 [00:08<00:00,  8.83s/it]\n",
            "100%|██████████| 1/1 [00:16<00:00, 16.43s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.74s/it]\n",
            "100%|██████████| 1/1 [00:09<00:00,  9.33s/it]\n",
            "100%|██████████| 1/1 [00:06<00:00,  6.76s/it]\n",
            "100%|██████████| 1/1 [00:23<00:00, 23.96s/it]\n",
            "100%|██████████| 1/1 [00:28<00:00, 28.57s/it]\n",
            "100%|██████████| 1/1 [00:13<00:00, 13.20s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.80s/it]\n",
            "100%|██████████| 1/1 [00:11<00:00, 11.80s/it]\n",
            "100%|██████████| 1/1 [00:58<00:00, 58.02s/it]\n",
            "100%|██████████| 1/1 [00:07<00:00,  7.43s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "id": "zqBzZEQcR2OQ",
        "outputId": "6299ee0f-43a4-4c70-f7a0-7e8ac18f99b3"
      },
      "source": [
        "#Loop to keep only generated text and add it as a new column in the dataframe\n",
        "my_generations=[]\n",
        "\n",
        "for i in range(len(generated_comments)):\n",
        "  a = test_set['Comments'][i].split()[-30:] #Get the matching string we want (30 words)\n",
        "  b = ' '.join(a)\n",
        "  c = ' '.join(generated_comments[i]) #Get all that comes after the matching string\n",
        "  my_generations.append(c.split(b)[-1])\n",
        "\n",
        "test_set['Generated_comments'] = my_generations\n",
        "\n",
        "\n",
        "#Finish the sentences when there is a point, remove after that\n",
        "final=[]\n",
        "\n",
        "for i in range(len(test_set)):\n",
        "  to_remove = test_set['Generated_comments'][i].split('.')[-1]\n",
        "  final.append(test_set['Generated_comments'][i].replace(to_remove,''))\n",
        "\n",
        "test_set['Generated_comments'] = final\n",
        "test_set.head()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>Comments</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Generated_comments</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>155948</td>\n",
              "      <td>/**   * Registers a {@link Closeable} resource...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/**   * Registers a {@link Closeable} resource...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>96982</td>\n",
              "      <td>/** * Create the frame. * Don't touch the code...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/** * Create the frame.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>130308</td>\n",
              "      <td>/**     * Return the information of the curren...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/**     * Return the information of the curren...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>429966</td>\n",
              "      <td>/** * This method was generated by Apache iBAT...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>/** * This method was generated by Apache iBAT...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>196813</td>\n",
              "      <td>/** * enum please.. */</td>\n",
              "      <td>NaN</td>\n",
              "      <td>\\n\\nreturn valid_types ( this, _this, get_vali...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    index  ...                                 Generated_comments\n",
              "0  155948  ...  /**   * Registers a {@link Closeable} resource...\n",
              "1   96982  ...                            /** * Create the frame.\n",
              "2  130308  ...  /**     * Return the information of the curren...\n",
              "3  429966  ...  /** * This method was generated by Apache iBAT...\n",
              "4  196813  ...  \\n\\nreturn valid_types ( this, _this, get_vali...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1niHTazpqjqT",
        "outputId": "73ee7cc9-5537-41f5-d5a0-2e83d3bbab45"
      },
      "source": [
        "print(test_set['Comments'][10])\n",
        "print(test_set['Generated_comments'][10])\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/** Returns an upper bound for the utility of refinements for the given hypothesis. */\n",
            " public function validateInput(Pipeline params) { if(params.length > 2) { return params.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60EToy6brqJY"
      },
      "source": [
        " public function validateInput(Pipeline params) { if(params.length > 2) { return params."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pwGilUfqz0c",
        "outputId": "693b0555-349b-408a-872a-e73d005409bf"
      },
      "source": [
        "#Using BLEU score to compare the real sentences with the generated ones\n",
        "import statistics\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "scores=[]\n",
        "\n",
        "for i in range(len(test_set)):\n",
        "  reference = test_set['Comments'][i]\n",
        "  candidate = test_set['Generated_comments'][i]\n",
        "  scores.append(sentence_bleu(reference, candidate))\n",
        "\n",
        "statistics.mean(scores)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4979932979543359"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwGDYT5Jsoe5",
        "outputId": "25f9d445-4e67-4a8e-a950-f7784ab02c46"
      },
      "source": [
        "print(test_set['Comments'])\n",
        "print(test_set['Generated_comments'])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0      /**   * Registers a {@link Closeable} resource...\n",
            "1      /** * Create the frame. * Don't touch the code...\n",
            "2      /**     * Return the information of the curren...\n",
            "3      /** * This method was generated by Apache iBAT...\n",
            "4                                 /** * enum please.. */\n",
            "                             ...                        \n",
            "195    /**     * This does BlueZ version detection   ...\n",
            "196    /**     * Validates <a href=\"https://issues.al...\n",
            "197    /**     * @see org.alfresco.repo.dictionary.Di...\n",
            "198    /**     * This method will be called whenever ...\n",
            "199    /**     * Triggers the request for a new api c...\n",
            "Name: Comments, Length: 200, dtype: object\n",
            "0      /**   * Registers a {@link Closeable} resource...\n",
            "1                                /** * Create the frame.\n",
            "2      /**     * Return the information of the curren...\n",
            "3      /** * This method was generated by Apache iBAT...\n",
            "4      \\n\\nreturn valid_types ( this, _this, get_vali...\n",
            "                             ...                        \n",
            "195                                                     \n",
            "196    /**     * Validates <a href=\"https://issues.al...\n",
            "197    /**     * @see org.alfresco.repo.dictionary.Di...\n",
            "198    /**     * This method will be called whenever ...\n",
            "199    /**     * Triggers the request for a new api c...\n",
            "Name: Generated_comments, Length: 200, dtype: object\n"
          ]
        }
      ]
    }
  ]
}